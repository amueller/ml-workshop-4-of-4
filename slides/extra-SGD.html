<!DOCTYPE html>
<html>
  <head>
    <title>Out-of-core and SGD</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="style.css">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

![:scale 40%](images/sklearn_logo.png)


### Advanced Machine Learning with scikit-learn Part I/II

# Stochastic Gradient Descent

Andreas C. Müller

Columbia University, scikit-learn

.smaller[https://github.com/amueller/ml-workshop-3-of-4]

---
# Stochastic Gradient Descent

( see http://leon.bottou.org/projects/sgd
and http://leon.bottou.org/papers/bottou-bousquet-2008
and http://scikit-learn.org/stable/modules/scaling_strategies.html )

???
N/A

---
class: right

# Decomposing Generalization Error

.center[
![:scale 100%](images/sgd_img_13.png)
]

Bottou et. al.

???
N/A

---
class: center, middle

.center[
![:scale 100%](images/sgd_img_14.png)
]

???
N/A

---
class: spacious
# The Trade-off

.center[
![:scale 100%](images/sgd_img_math_1.png)
]
- If `$n_\text{max}$` is large, we are constraint by $T_\max$!
- Use a worse algorithm but look at more data!


???
- Making the optimization error small doesn’t matter if it means we can’t look at all training
examples!
- Trade optimization error for estimation error

---
# Reminder: Gradient Descent

.right-column[![:scale 1-0%](images/sgd_img_15.png)]

Want: $$\arg \min_w F(w)$$

Initialize $w_0$

$$w^{(i+1)} \leftarrow w^{(i)} - \eta_i\frac{d}{dw}F(w^{(i)})$$

Converges to local minimum

???

---

# Reminder: Gradient Descent

.center[
![:scale 40%](images/sgd_img_16.png)
]

$$w^{(i+1)} \leftarrow w^{(i)} - \eta_i\frac{d}{dw}F(w^{(i)})$$


???


---

# Pick a learning rate

.center[
![:scale 90%](images/sgd_img_17.png)
]

$$w^{(i+1)} \leftarrow w^{(i)} - \eta_i\frac{d}{dw}F(w^{(i)})$$

???

---

# Stochastic Gradient Descent

- Logistic Regression:

`$$F(w) = -C \sum_{i=1}^n\log(\exp(-y_iw^T \textbf{x}_i) +1 ) + ||w||_2^2$$`


- Pick $x_i$ randomly, then

`$$\frac{d}{dw}F(w) = \frac{d}{dw} -C \log(\exp(-y_iw^T \textbf{x}_i) +1 ) + \frac{1}{n}||w||_2^2$$`

- In practice: just iterate over i.


???
Is a stochastic approximation of gradient of F with expectation the actual gradient.

---
class: spacious

# SGD and partial_fit

- SGDClassifier(), SGDRegressor() fast on very large datasets
- Tuning learning rate and schedule can be tricky.
- partial_fit allows working with out-of-memory data!


???
- SGDClassifier(), SGDRegressor() fast on very large datasets – have many different loss and regularization options.
- Tuning learning rate and schedule can be tricky. “optimal” learning rate only works with unitvariance data. Averaging can be helpful.
- partial_fit allows working with out-of-memory data!


---
class: center, spacious

![:scale 90%](images/sgd_img_18.png)

![:scale 90%](images/sgd_img_19.png)


    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
